# DeepSeek-OCR ê³µì‹ API í†µí•© ë¦¬íŒ©í† ë§ ê³„íš

**ì‘ì„±ì¼**: 2025-10-28
**ëª©í‘œ**: ê³µì‹ APIë¥¼ ì •í™•íˆ ë”°ë¥´ë©´ì„œ ê¸°ì¡´ ê¸°ëŠ¥ 100% ë³´ì¡´
**ìƒíƒœ**: ê³„íš ë‹¨ê³„

---

## ğŸ¯ í”„ë¡œì íŠ¸ í•µì‹¬ ê¸°ëŠ¥ (ë³´ì¡´ í•„ìˆ˜)

### ê¸°íšëœ í•µì‹¬ ì•„í‚¤í…ì²˜
1. **2-Pass íŒŒì´í”„ë¼ì¸**
   - Pass 1: `<|grounding|>` ëª¨ë“œë¡œ í˜ì´ì§€ ì „ì²´ êµ¬ì¡° ë¶„ì„ â†’ Bounding Box ê²€ì¶œ
   - Pass 2: ê²€ì¶œëœ ìš”ì†Œë³„ ìƒì„¸ ë¶„ì„ (crop â†’ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸)

2. **7-Category ìš”ì†Œ ë¶„ë¥˜**
   - text_header, text_section, text_paragraph
   - table, graph, diagram, complex_image

3. **DocJSON ìƒì„±**
   - í•œêµ­ì–´ ì² ê°• ê¸°ìˆ ë¬¸ì„œ íŠ¹í™” ([í•­ëª©], [í‚¤ì›Œë“œ], [ìì—°ì–´ ìš”ì•½])
   - ì„¹ì…˜ íŠ¸ë¦¬ êµ¬ì¡° (numbering ê¸°ë°˜ ê³„ì¸µ)
   - ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì €ì¥ (graph, table, diagram ë“±)

4. **RTX 4060 8GB ìµœì í™”**
   - float16/bfloat16 dtype
   - ë©”ëª¨ë¦¬ ê´€ë¦¬ (@torch.no_grad(), CUDA cache clearing)
   - Lazy loading, ëª¨ë¸ unload

---

## ğŸ” í˜„ì¬ ë¬¸ì œ ë¶„ì„

### 1. í”„ë¡¬í”„íŠ¸ í¬ë§· ë¬¸ì œ (CRITICAL)

**í˜„ì¬ ì½”ë“œ (deepseek_engine.py:109):**
```python
formatted_prompt = f"<ï½œUserï½œ>: {prompt}\n\n<ï½œAssistantï½œ>:"
# âŒ ì „ê° ë¬¸ì ì‚¬ìš© (ì˜ëª»ë¨!)
# âŒ ìˆ˜ë™ í¬ë§·íŒ…
```

**ê³µì‹ êµ¬í˜„ (modeling_deepseekocr.py:710-722, 741):**
```python
conversation = [
    {
        "role": "<|User|>",  # âœ… ë°˜ê° ë¬¸ì
        "content": f'{prompt}',  # ì´ë¯¸ <image> í¬í•¨
        "images": [f'{image_file}'],
    },
    {"role": "<|Assistant|>", "content": ""},
]
prompt = format_messages(conversations=conversation, sft_format='plain', system_prompt='')
```

**ì˜í–¥ë„**:
- ğŸ”´ CRITICAL: ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•  ê°€ëŠ¥ì„±
- ğŸ”´ ì „ì²´ Pass 1, Pass 2 ì¶”ë¡ ì— ì˜í–¥

### 2. ì´ë¯¸ì§€ ì „ë‹¬ ë°©ì‹ ë¬¸ì œ

**í˜„ì¬:**
```python
response = self.model.infer(
    prompt=formatted_prompt,  # ì˜ëª»ëœ í¬ë§·
    image_file=image,         # PIL Image ê°ì²´
)
```

**ê³µì‹:**
- conversationì— images ë¦¬ìŠ¤íŠ¸ í¬í•¨
- `load_pil_images(conversation)` í•¨ìˆ˜ë¡œ ì¶”ì¶œ
- `dynamic_preprocess()` + `image_transform()` ì „ì²˜ë¦¬

**ì˜í–¥ë„**:
- ğŸŸ¡ MEDIUM: ëª¨ë¸ì€ ì‘ë™í•˜ì§€ë§Œ ì „ì²˜ë¦¬ ëˆ„ë½
- ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™” ë¶ˆê°€

### 3. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ëˆ„ë½

**ê³µì‹ êµ¬í˜„ (modeling_deepseekocr.py:773-850):**
```python
# 1. Dynamic preprocessing (large image â†’ crops)
if crop_mode and (image.size[0] > 640 or image.size[1] > 640):
    images_crop_raw, crop_ratio = dynamic_preprocess(image)

# 2. Global view with padding
global_view = ImageOps.pad(image, (base_size, base_size))

# 3. Normalization
image_transform = BasicImageTransform(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
images_list.append(image_transform(global_view).to(torch.bfloat16))

# 4. Local views (cropped patches)
for crop in images_crop_raw:
    images_crop_list.append(image_transform(crop).to(torch.bfloat16))
```

**í˜„ì¬ ìƒí™©**:
- âŒ ìœ„ ì „ì²˜ë¦¬ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- PIL Imageë¥¼ ì§ì ‘ ì „ë‹¬

**ì˜í–¥ë„**:
- ğŸŸ¡ MEDIUM: ê³ í•´ìƒë„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œ ì„±ëŠ¥ ì €í•˜
- 1280Ã—1280 ì´ìƒì—ì„œ í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥

---

## ğŸ“‹ ë¦¬íŒ©í† ë§ ì „ëµ

### ì›ì¹™
1. âœ… **ê¸°ëŠ¥ ë³´ì¡´**: 2-Pass, 7-Category, DocJSON ìƒì„± ë¡œì§ 100% ìœ ì§€
2. âœ… **ì ì§„ì  ìˆ˜ì •**: í•œ ë²ˆì— í•˜ë‚˜ì”©, í…ŒìŠ¤íŠ¸ í›„ ì»¤ë°‹
3. âœ… **ê³µì‹ API ì¤€ìˆ˜**: modeling_deepseekocr.pyì˜ infer() ì‚¬ìš© ë°©ì‹ ì •í™•íˆ ë”°ë¦„
4. âœ… **í•˜ìœ„ í˜¸í™˜ì„±**: ê¸°ì¡´ Config, í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ìµœëŒ€í•œ ìœ ì§€

### ì ‘ê·¼ ë°©ë²•: **Wrapper Layer íŒ¨í„´**

ê³µì‹ infer() ë©”ì„œë“œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, ìš°ë¦¬ì˜ 2-Pass ë¡œì§ì„ ìœ„í•œ wrapper ìƒì„±

```python
# ê³µì‹ APIëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
model.infer(tokenizer, prompt, image_file, ...)

# ìš°ë¦¬ê°€ ë§Œë“œëŠ” ê²ƒ
class DeepSeekEngine:
    def infer(self, image, prompt):
        # ê³µì‹ í¬ë§·ìœ¼ë¡œ ë³€í™˜
        response = self.model.infer(
            self.tokenizer,
            prompt=prompt,  # ì´ë¯¸ <image> íƒœê·¸ í¬í•¨
            image_file=image,
            base_size=self.config.base_size,
            image_size=self.config.image_size,
            crop_mode=self.config.crop_mode,
        )
        return response
```

---

## ğŸ”§ êµ¬ì²´ì  ìˆ˜ì • ê³„íš

### Phase 1: í”„ë¡¬í”„íŠ¸ í¬ë§· ìˆ˜ì • (CRITICAL)

**íŒŒì¼**: `src/deepseek_ocr/engine/deepseek_engine.py`

**í˜„ì¬ (109ë²ˆ ë¼ì¸):**
```python
formatted_prompt = f"<ï½œUserï½œ>: {prompt}\n\n<ï½œAssistantï½œ>:"
```

**ìˆ˜ì • í›„:**
```python
# í”„ë¡¬í”„íŠ¸ëŠ” ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ (<image>\n<|grounding|>...)
# ê³µì‹ infer()ê°€ ë‚´ë¶€ì—ì„œ format_messages() í˜¸ì¶œ
# ìš°ë¦¬ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ!
```

**ë³€ê²½ ì‚¬í•­**:
- âŒ ì‚­ì œ: ìˆ˜ë™ í¬ë§·íŒ… (109ë²ˆ ë¼ì¸)
- âœ… ìœ ì§€: í”„ë¡¬í”„íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬ (prompts.pyëŠ” ì´ë¯¸ ì˜¬ë°”ë¦„)

**í…ŒìŠ¤íŠ¸**:
```python
# prompts.pyì˜ STRUCTURE_ANALYSIS_PROMPT í™•ì¸
assert prompt.startswith("<image>")
assert "<|grounding|>" in prompt
# âœ… ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹!
```

### Phase 2: infer() ë©”ì„œë“œ ë‹¨ìˆœí™”

**íŒŒì¼**: `src/deepseek_ocr/engine/deepseek_engine.py`

**í˜„ì¬ (114-121ë²ˆ ë¼ì¸):**
```python
response = self.model.infer(
    tokenizer=self.tokenizer,
    prompt=formatted_prompt,  # âŒ ì˜ëª»ëœ í¬ë§·
    image_file=image,
    base_size=self.config.base_size,
    image_size=self.config.image_size,
    crop_mode=self.config.crop_mode,
)
```

**ìˆ˜ì • í›„:**
```python
response = self.model.infer(
    tokenizer=self.tokenizer,
    prompt=prompt,  # âœ… prompts.pyì—ì„œ ì˜¨ ì›ë³¸ ê·¸ëŒ€ë¡œ
    image_file=image,  # PIL Image ê°ì²´ (ê³µì‹ API ì§€ì›)
    base_size=self.config.base_size,
    image_size=self.config.image_size,
    crop_mode=self.config.crop_mode,
)
```

**ë³€ê²½ ì‚¬í•­**:
- âŒ ì‚­ì œ: `formatted_prompt` ë³€ìˆ˜ ìƒì„± (109ë²ˆ ë¼ì¸)
- âœ… ìˆ˜ì •: `prompt=prompt` (ì›ë³¸ ê·¸ëŒ€ë¡œ ì „ë‹¬)

### Phase 3: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê²€ì¦ (ì„ íƒì‚¬í•­)

**í˜„ì¬ ìƒí™©**:
- ê³µì‹ infer()ê°€ ë‚´ë¶€ì—ì„œ `dynamic_preprocess()` ìë™ ìˆ˜í–‰
- crop_mode=True ì„¤ì •ìœ¼ë¡œ í™œì„±í™”ë¨

**í™•ì¸ í•„ìš”**:
```python
# modeling_deepseekocr.py:773-785
if crop_mode:
    if image.size[0] > 640 or image.size[1] > 640:
        images_crop_raw, crop_ratio = dynamic_preprocess(image)
```

**ìš°ë¦¬ ì„¤ì • (config.py):**
```python
crop_mode: bool = True  # âœ… ì´ë¯¸ í™œì„±í™”ë¨!
base_size: int = 1024   # global view í¬ê¸°
image_size: int = 640   # local view í¬ê¸°
```

**ê²°ë¡ **:
- âœ… ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë¨
- ê³µì‹ infer()ê°€ ë‚´ë¶€ì—ì„œ ì „ì²˜ë¦¬ ìë™ ìˆ˜í–‰
- **ì¶”ê°€ ì‘ì—… ë¶ˆí•„ìš”**

---

## âœ… ìˆ˜ì • ì „/í›„ ë¹„êµ

### Before (í˜„ì¬ - ì˜¤ë¥˜ ë°œìƒ)
```python
# deepseek_engine.py:106-121
def infer(self, image, prompt):
    self._load_model()

    # âŒ ì˜ëª»ëœ í¬ë§·íŒ…
    formatted_prompt = f"<ï½œUserï½œ>: {prompt}\n\n<ï½œAssistantï½œ>:"

    try:
        response = self.model.infer(
            tokenizer=self.tokenizer,
            prompt=formatted_prompt,  # âŒ ì „ê° ë¬¸ì
            image_file=image,
            base_size=self.config.base_size,
            image_size=self.config.image_size,
            crop_mode=self.config.crop_mode,
        )
        return response
    finally:
        if self._device == "cuda":
            torch.cuda.empty_cache()
```

### After (ìˆ˜ì • í›„ - ê³µì‹ API ì¤€ìˆ˜)
```python
# deepseek_engine.py:106-119
def infer(self, image, prompt):
    """
    Generic inference using official DeepSeek-OCR API.

    Args:
        image: PIL Image
        prompt: Already formatted prompt from prompts.py
                (e.g., "<image>\n<|grounding|>...")

    Returns:
        Model response string
    """
    self._load_model()

    try:
        # Use official API - prompt is already correctly formatted
        # Official infer() handles:
        # 1. format_messages() - converts to conversation format
        # 2. load_pil_images() - extracts images
        # 3. dynamic_preprocess() - image preprocessing
        # 4. forward() - model inference
        response = self.model.infer(
            tokenizer=self.tokenizer,
            prompt=prompt,  # âœ… Original prompt from prompts.py
            image_file=image,  # PIL Image (official API accepts this)
            base_size=self.config.base_size,
            image_size=self.config.image_size,
            crop_mode=self.config.crop_mode,
        )
        return response
    finally:
        if self._device == "cuda":
            torch.cuda.empty_cache()
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### Test 1: í”„ë¡¬í”„íŠ¸ í¬ë§· ê²€ì¦
```python
from src.deepseek_ocr.engine.prompts import STRUCTURE_ANALYSIS_PROMPT

# ê²€ì¦: í”„ë¡¬í”„íŠ¸ê°€ ê³µì‹ í¬ë§·ì¸ê°€?
assert STRUCTURE_ANALYSIS_PROMPT.startswith("<image>")
assert "<|grounding|>" in STRUCTURE_ANALYSIS_PROMPT
assert "<ï½œUserï½œ>" not in STRUCTURE_ANALYSIS_PROMPT  # ì „ê° ë¬¸ì ì—†ìŒ
print("âœ… Prompt format is correct!")
```

### Test 2: Pass 1 ë‹¨ì¼ í˜ì´ì§€ í…ŒìŠ¤íŠ¸
```python
from PIL import Image
from src.deepseek_ocr.engine.deepseek_engine import DeepSeekEngine
from src.deepseek_ocr.core.config import load_config

config = load_config(preset="rtx4090")
engine = DeepSeekEngine(config)

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
image = Image.open("test_page.png")

# Pass 1: êµ¬ì¡° ë¶„ì„
from src.deepseek_ocr.engine.prompts import STRUCTURE_ANALYSIS_PROMPT
response = engine.infer(image, STRUCTURE_ANALYSIS_PROMPT)

# ì‘ë‹µ í™•ì¸
print(response)
assert "elements" in response or "bbox" in response
print("âœ… Pass 1 inference works!")
```

### Test 3: Pass 2 ìš”ì†Œ ë¶„ì„ í…ŒìŠ¤íŠ¸
```python
from src.deepseek_ocr.engine.prompts import get_element_prompt

# í…Œì´ë¸” ìš”ì†Œ í…ŒìŠ¤íŠ¸
cropped_table = Image.open("cropped_table.png")
table_prompt = get_element_prompt("table", context="ë…¸ì—´ ê´€ë¦¬ ê¸°ì¤€í‘œ")

response = engine.infer(cropped_table, table_prompt)
print(response)
assert "items" in response or "markdown" in response
print("âœ… Pass 2 inference works!")
```

### Test 4: ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸
```python
# RunPodì—ì„œ ì‹¤í–‰
python runpod/process.py --input test_pdfs/ --output outputs/ --preset rtx4090

# ê²°ê³¼ í™•ì¸
# 1. outputs/test_docjson.json ìƒì„±ë¨
# 2. outputs/cropped_images/ ì— ì´ë¯¸ì§€ ì €ì¥ë¨
# 3. ì—ëŸ¬ ì—†ì´ ì™„ë£Œ
print("âœ… Full pipeline works!")
```

---

## ğŸ“Š ìœ„í—˜ë„ í‰ê°€

| ìˆ˜ì • í•­ëª© | ìœ„í—˜ë„ | ì˜í–¥ ë²”ìœ„ | ë¡¤ë°± ê°€ëŠ¥ì„± |
|----------|--------|----------|------------|
| í”„ë¡¬í”„íŠ¸ í¬ë§· ìˆ˜ì • | ğŸ”´ HIGH | ì „ì²´ ì¶”ë¡  | âœ… ì‰¬ì›€ |
| infer() ë‹¨ìˆœí™” | ğŸŸ¡ MEDIUM | Engineë§Œ | âœ… ì‰¬ì›€ |
| ì´ë¯¸ì§€ ì „ì²˜ë¦¬ | ğŸŸ¢ LOW | ì—†ìŒ (ì´ë¯¸ êµ¬í˜„ë¨) | N/A |

---

## ğŸš€ ì‹¤í–‰ ìˆœì„œ

1. **ë°±ì—… ìƒì„±**
   ```bash
   git commit -m "Backup before refactoring"
   cp src/deepseek_ocr/engine/deepseek_engine.py src/deepseek_ocr/engine/deepseek_engine.py.backup
   ```

2. **ìˆ˜ì • 1: í”„ë¡¬í”„íŠ¸ í¬ë§· ì œê±°**
   - 109ë²ˆ ë¼ì¸ ì‚­ì œ
   - 116ë²ˆ ë¼ì¸ ìˆ˜ì • (`formatted_prompt` â†’ `prompt`)
   - ë¡œì»¬ í…ŒìŠ¤íŠ¸

3. **ìˆ˜ì • 2: ì£¼ì„ ë° ë¬¸ì„œí™”**
   - docstring ì—…ë°ì´íŠ¸
   - ê³µì‹ API ì‚¬ìš© ëª…ì‹œ

4. **ì»¤ë°‹ ë° í‘¸ì‹œ**
   ```bash
   git add src/deepseek_ocr/engine/deepseek_engine.py
   git commit -m "Fix prompt formatting to match official DeepSeek-OCR API"
   git push origin master
   ```

5. **RunPod í…ŒìŠ¤íŠ¸**
   ```bash
   cd /workspace/SUT_DOCR-ANALYZER
   git pull origin master
   source .venv/bin/activate
   python runpod/process.py --input test_pdfs/ --output outputs/ --preset rtx4090
   ```

6. **ê²€ì¦**
   - Pass 1 ê²°ê³¼ í™•ì¸ (elements, bbox ê²€ì¶œ)
   - Pass 2 ê²°ê³¼ í™•ì¸ (í•­ëª©, í‚¤ì›Œë“œ, ìš”ì•½ ì¶”ì¶œ)
   - DocJSON ìƒì„± í™•ì¸
   - ì´ë¯¸ì§€ ì €ì¥ í™•ì¸

---

## âœ… ì„±ê³µ ê¸°ì¤€

### ê¸°ëŠ¥ ë³´ì¡´ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Pass 1: í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ (bbox ê²€ì¶œ) ì‘ë™
- [ ] Pass 2: ìš”ì†Œë³„ ìƒì„¸ ë¶„ì„ ì‘ë™
- [ ] 7-Category ë¶„ë¥˜ ì •í™•ë„ ìœ ì§€
- [ ] DocJSON ìƒì„± (í•­ëª©, í‚¤ì›Œë“œ, ìš”ì•½)
- [ ] ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì €ì¥
- [ ] ì„¹ì…˜ íŠ¸ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] RTX 4060 8GB ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ < 7.5GB

### ì„±ëŠ¥ ê°œì„  ì§€í‘œ
- [ ] ì¶”ë¡  ì—ëŸ¬ 0%
- [ ] í”„ë¡¬í”„íŠ¸ ì¸ì‹ë¥  í–¥ìƒ
- [ ] ê³ í•´ìƒë„ ë¬¸ì„œ ì²˜ë¦¬ í’ˆì§ˆ í–¥ìƒ (1280Ã—1280)

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„ (ìˆ˜ì • í›„)

1. **í•œêµ­ì–´ ì„±ëŠ¥ ê²€ì¦**
   - í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì •í™•ë„ í…ŒìŠ¤íŠ¸
   - ì„¹ì…˜ ë²ˆí˜¸ íŒŒì‹± ê²€ì¦ (1., 1.1., 1.1.1.)

2. **ë³µì¡ë„ ì²˜ë¦¬ ê°œì„ **
   - complex_image ë¶„ë¥˜ ì •í™•ë„ í–¥ìƒ
   - ë³µì¡í•œ í‘œ/ê·¸ë˜í”„ ì²˜ë¦¬

3. **ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”**
   - ë‹¤ì¤‘ ë¬¸ì„œ ì²˜ë¦¬ ì„±ëŠ¥
   - ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°œì„ 

---

**ì‘ì„±ì**: Claude (DeepSeek-OCR Expert)
**ê²€í† ì**: Human (Project Owner)
**ìŠ¹ì¸**: ë¦¬íŒ©í† ë§ ì‹¤í–‰ ì „ ìµœì¢… ê²€í†  í•„ìš”
